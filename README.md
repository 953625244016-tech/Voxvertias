ğŸ›¡ï¸ AI-Generated Voice Detection System
Cross-Language Deepfake Audio Classifier (Multi-Lingual)
This production-ready system detects whether an audio sample is generated by AI (Synthetic) or a real Human voice. It is specifically designed to handle the nuances of five major languages: Tamil, English, Hindi, Malayalam, and Telugu.


ğŸš€ Key Features
Deep Learning Core: Uses a custom Convolutional Neural Network (CNN) built with PyTorch.
Advanced Audio Processing: Converts raw audio into Log-Mel Spectrograms to detect synthetic frequency artifacts.
Modular Architecture: Professional separation of concerns (API, Services, ML Models, Frontend).
Interactive Dashboard: Built with Streamlit for real-time file uploads and detection history.
REST API: High-performance FastAPI backend with Base64 support.
Language Agnostic: Analyzes acoustic textures rather than words, making it effective across different languages.


ğŸ› ï¸ Tech Stack
Language: Python 3.10+
Machine Learning: PyTorch, Librosa (Audio Analysis)
Backend: FastAPI, Uvicorn
Frontend: Streamlit
Data Handling: NumPy, Pandas
Environment: Docker, Virtual Environments


ğŸ“‚ Project Structure
code
Text
voice_detector/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/            # FastAPI Route definitions
â”‚   â”œâ”€â”€ core/           # Configuration and constants
â”‚   â”œâ”€â”€ models/         # PyTorch CNN Architecture
â”‚   â”œâ”€â”€ services/       # Audio Engine and Inference logic
â”‚   â””â”€â”€ frontend/       # Streamlit Dashboard code
â”œâ”€â”€ data/               # Dataset (train/val) for human/ai voices
â”œâ”€â”€ weights/            # Saved model weights (.pth)
â”œâ”€â”€ main.py             # Backend Entry point
â”œâ”€â”€ train.py            # ML Training script
â”œâ”€â”€ requirements.txt    # Project dependencies
â””â”€â”€ Dockerfile          # Containerization logic


âš™ï¸ Installation & Setup
1. Prerequisites
Python 3.10 or higher
FFmpeg (Required for audio processing)
Windows: choco install ffmpeg or download from ffmpeg.org
Linux: sudo apt install ffmpeg

2. Environment Setup
code
Bash
# Create a virtual environment
python -m venv venv

# Activate it (Windows)
.\venv\Scripts\activate

# Activate it (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

3. Initialize/Train the Model
Place your audio samples in data/train/human and data/train/ai, then run:
code
Bash
python train.py
ğŸƒ How to Run
Method 1: Local Execution (Two Terminals)
Terminal 1 (Backend):
code
Bash
python main.py
Terminal 2 (Frontend UI):
code
Bash
python -m streamlit run app/frontend/dashboard.py
Method 2: Docker Execution
code
Bash
docker build -t voice-detector .
docker run -p 8000:8000 -p 8501:8501 voice-detector


ğŸ“¡ API Documentation
Once the backend is running, visit http://localhost:8000/docs for the interactive Swagger UI.
Endpoint: POST /api/v1/detect

Request Body:
code
JSON
{
  "audio_b64": "UklGRuY6AABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAA..."
}
Response Body:
code
JSON
{
  "prediction": "AI_GENERATED",
  "confidence": 0.9421
}


ğŸ§  ML Pipeline Explanation
Preprocessing: Audio is resampled to 16kHz and standardized to a 4-second duration.
Feature Extraction: Time-domain signals are converted to 2D Mel-Spectrogram images (128 frequency bands).
Inference: A CNN model analyzes the spectrogram. AI voices often leave "checkerboard" artifacts or overly smooth frequency lines which the model identifies.
Output: A Softmax layer provides the final probability score between 0 and 1.


ğŸ“œ License
This project is developed for hackathon purposes. All rights reserved.
Author: Hariharan P
        Akshay D
        AbishekRaj A
        Abishek 
Project: AI Voice Guard 0.1